{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Image classification of wheat plant disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was my final project to achieve the title of compute engineer. \n",
    "In this blog i'll explain how i did it and which tools i used.\n",
    "\n",
    "In this project i created my own image dataset to work with.\n",
    "Some of the code used is on [github](https://github.com/fawolfmann/Tesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was created with the help of [INTA](https://www.argentina.gob.ar/inta) Argentinian agriculture institute and received a scholarship of [SECyT](https://www.unc.edu.ar/ciencia-y-tecnolog%c3%ada/) Argentinian University Technology and Science secretary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "image_base=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem\n",
    "\n",
    "The problem we pose is the lose of grain on wheat harvest caused by leaf disease. \n",
    "\n",
    "In the next images we see the kind of leaf disease that are more common in the wheat. \n",
    "\n",
    "| Healthy        | Mildew           | Leaf Spot  | Rust |\n",
    "| :-------------: |:-------------:| :-----:| :-----: |\n",
    "| ![](https://fawolfmann.github.io/aboutme/images/tesis/problem1.png) | ![](https://fawolfmann.github.io/aboutme/images/tesis/aboutme/images/tesis/problem2.png) | ![](https://fawolfmann.github.io/aboutme/images/tesis/aboutme/images/tesis/problem3.png) | ![](https://fawolfmann.github.io/aboutme/images/tesis/aboutme/images/tesis/problem4.png)|\n",
    "\n",
    "As you can see in the images they are slightly different, with the idea is to classify this images and define if a leaf have a disease. \n",
    "\n",
    "So now we have defined our problem next we will see wich technic we used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions neural networks\n",
    "For the image classification we will use CNN or Convolutions neural networks. \n",
    "This is not a CNN course but will make a little intro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Architecture\n",
    "![](https://fawolfmann.github.io/aboutme/images/tesis/aboutme/images/tesis/cnn_architecture.png)\n",
    "Image from [Google](https://developers.google.com/machine-learning/practica/image-classification/convolutional-neural-networks)\n",
    "\n",
    "\n",
    "A CNN basically is made of 2 modules, the first is the convolutional module that is used to create a feature map of an image. It made convolution with the patterns it was trained, in the firsts layers it look for basics patters ass lines or circles but in the deep convolutions it look for more complex patterns as a leg or a face in the case of a CNN trained to classify people. \n",
    "\n",
    "\n",
    "The second module is the classification module, its a dense layer or fully connected layer that use the feature map as input and give a classification as an output, it trains the connection weights to classify.\n",
    "\n",
    "This is a very short intro if you want to get a deeper knowleadge of computer vision i recomnend this [Standford Course](http://cs231n.stanford.edu/) contains the slides and syllabus and the YouTube [videos](https://www.youtube.com/watch?v=vT1JzLTH4G4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of dataset\n",
    "We create an image dataset crawling image from the web and taking photos in the field. After the images where validated and labeled we got:\n",
    "\n",
    "\n",
    "| Label        \t| Amount     \t|\n",
    "| :----------:\t|:----------:\t| \n",
    "| Healthy\t\t| 113 \t\t\t| \n",
    "| Mildew\t\t| 102\t\t    |   \n",
    "| Leaf Spot\t\t| 122   \t\t|   \n",
    "| Rust\t\t\t| 159\t\t\t|\n",
    "\n",
    "This is a small amount of pictures to work with but we handle to generate a good solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "To the training part I install everything on a server with a Titan X GPU \n",
    "\n",
    "I used docker with a Tensorflow 2.0 image and run everything on Jupyters notebooks. All the code was build with Keras. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of models\n",
    "In the training steps I made 3 iteration to get the final results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 1\n",
    "First I used my dataset on transfer learning with several pretrained models, trained with Imagenet, this is easly doit with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.xception.Xception(\n",
    "                                                   input_shape=IMG_SHAPE,\n",
    "                                                   include_top=False,\n",
    "                                                   weights='imagenet'\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you have to define the shape of the input usualy 224x224 or 512x512, and define if you want to get the top layer, the clasification module, with all the Imagenet classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this iteration we also used [Data augmentation](https://bair.berkeley.edu/blog/2019/06/07/data_aug/), keras give you a good tool to work with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
    "                                                                  validation_split=0.15,                            \n",
    "                                                                  zoom_range=0.4,\n",
    "                                                                  horizontal_flip=True,\n",
    "                                                                  rotation_range=90\n",
    "                                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you define the way that keras will augment your images, all the configuration can be fund [here](https://keras.io/api/preprocessing/image/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this iteration wasn't succesfull, the maximum accuracy achived was **65.8%** on the validation dataset, not even the test dataset. \n",
    "So i keep training with diferent ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 2\n",
    "So then i think why my model went wrong, and talking with some professor of my university the y told me to look for a base dataset to make the all the full training and i found [PlantVillage](https://www.kaggle.com/emmarex/plantdisease) a very similar dataset, it contains 50.000+ images of plant leaf with and without diseases.  \n",
    "\n",
    "After making the full training with this new dataset, I made the transfer learning with my own dataset. \n",
    "\n",
    "Unfortunatly i didnt get so much better results.  With a maximum of **72.3%** of accuracy on validation dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Android app implementation\n",
    "After we trained successfully a model we convert it with TensorflowLite and created an Android app to use this trained model and make predictions on the device.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this work i realased that the most time cosuming task was to manipulate the data to make it work with your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}